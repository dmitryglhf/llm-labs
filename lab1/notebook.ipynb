{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Creating tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "DDGSResult = list[dict[str, str]]\n",
    "\n",
    "@tool\n",
    "def search(\n",
    "    query: str,\n",
    "    max_results: int = 10,\n",
    "    region: str = \"us-en\",\n",
    "    safesearch: str = \"moderate\",\n",
    "    timelimit: str | None = None,\n",
    "    page: int = 1,\n",
    "    backend: str = \"auto\",\n",
    ") -> DDGSResult | str:\n",
    "    \"\"\"\n",
    "    DuckDuckGo text search for web pages, articles, and information.\n",
    "\n",
    "    Args:\n",
    "        query: text search query.\n",
    "        region: us-en, uk-en, ru-ru, etc. Defaults to us-en.\n",
    "        safesearch: on, moderate, off. Defaults to \"moderate\".\n",
    "        timelimit: d, w, m, y. Defaults to None.\n",
    "        max_results: maximum number of results. Defaults to 10.\n",
    "        page: page of results. Defaults to 1.\n",
    "        backend: A single or comma-delimited backends. Defaults to \"auto\".\n",
    "    Returns:\n",
    "        List of dictionaries with search results.\n",
    "    \"\"\"\n",
    "    logger.info(f\"search: {query[:50]}...\")\n",
    "    results = DDGS().text(\n",
    "        query=query,\n",
    "        region=region,\n",
    "        safesearch=safesearch,\n",
    "        timelimit=timelimit,\n",
    "        max_results=max_results,\n",
    "        page=page,\n",
    "        backend=backend,\n",
    "    )\n",
    "    logger.info(f\"search: {len(results)} results\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Arxiv search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def arxiv_search(query: str, max_results: int = 3) -> list[dict]:\n",
    "    \"\"\"Search for relevant papers using the arXiv tool\"\"\"\n",
    "    logger.info(f\"arxiv_search: {query[:50]}...\")\n",
    "    search = arxiv.Search(\n",
    "        query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = []\n",
    "    for result in search.results():\n",
    "        papers.append(\n",
    "            {\n",
    "                \"title\": result.title,\n",
    "                \"authors\": [author.name for author in result.authors],\n",
    "                \"summary\": result.summary,\n",
    "                \"published\": result.published.isoformat(),\n",
    "                \"url\": result.entry_id,\n",
    "                \"arxiv_id\": result.entry_id.split(\"/\")[-1],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    logger.info(f\"arxiv_search: {len(papers)} papers\")\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Pydantic models and State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from textwrap import dedent\n",
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchQuery(BaseModel):\n",
    "    topic: str = Field(..., description=\"Research topic to investigate\")\n",
    "    max_papers: int = Field(\n",
    "        default=5, description=\"Maximum number of arXiv papers to retrieve\"\n",
    "    )\n",
    "    max_web_results: int = Field(\n",
    "        default=5, description=\"Maximum number of web results to retrieve\"\n",
    "    )\n",
    "    year_filter: str | None = Field(\n",
    "        default=None, description=\"Optional year filter for papers\"\n",
    "    )\n",
    "\n",
    "class ResearchPlan(BaseModel):\n",
    "    arxiv_query: str = Field(..., description=\"Formatted query for arXiv search\")\n",
    "    web_queries: list[str] = Field(..., description=\"List of web search queries\")\n",
    "    focus_areas: list[str] = Field(\n",
    "        ..., description=\"Key focus areas for the research\"\n",
    "    )\n",
    "\n",
    "class ArxivFinding(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the paper\")\n",
    "    authors: list[str] = Field(..., description=\"List of author names\")\n",
    "    summary: str = Field(..., description=\"Summary of the paper\")\n",
    "    url: str = Field(..., description=\"ArXiv URL\")\n",
    "    published: str = Field(..., description=\"Publication date\")\n",
    "\n",
    "class ArxivFindings(BaseModel):\n",
    "    papers: list[ArxivFinding] = Field(\n",
    "        default_factory=list, description=\"List of found arXiv papers\"\n",
    "    )\n",
    "\n",
    "class WebFinding(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the web source\")\n",
    "    url: str = Field(..., description=\"URL of the web source\")\n",
    "    content_summary: str = Field(..., description=\"Summary of the content\")\n",
    "\n",
    "class WebFindings(BaseModel):\n",
    "    sources: list[WebFinding] = Field(\n",
    "        default_factory=list, description=\"List of found web sources\"\n",
    "    )\n",
    "\n",
    "class ResearchReport(BaseModel):\n",
    "    topic: str = Field(..., description=\"Research topic\")\n",
    "    key_findings: list[str] = Field(..., description=\"List of key findings\")\n",
    "    summary: str = Field(..., description=\"Overall summary of the research\")\n",
    "    gaps_identified: list[str] = Field(\n",
    "        default_factory=list, description=\"Identified research gaps\"\n",
    "    )\n",
    "\n",
    "class ReviewFeedback(BaseModel):\n",
    "    approved: bool = Field(..., description=\"Whether the report is approved\")\n",
    "    missing_aspects: list[str] = Field(\n",
    "        default_factory=list, description=\"Missing aspects in the report\"\n",
    "    )\n",
    "    quality_score: float = Field(..., description=\"Quality score (0.0-1.0)\")\n",
    "    suggestions: list[str] = Field(\n",
    "        default_factory=list, description=\"Suggestions for improvement\"\n",
    "    )\n",
    "\n",
    "class ResearchState(BaseModel):\n",
    "    query: ResearchQuery\n",
    "    plan: ResearchPlan | None = None\n",
    "    arxiv_findings: Annotated[list[ArxivFinding], operator.add] = Field(\n",
    "        default_factory=list\n",
    "    )\n",
    "    web_findings: Annotated[list[WebFinding], operator.add] = Field(\n",
    "        default_factory=list\n",
    "    )\n",
    "    report: ResearchReport | None = None\n",
    "    review: ReviewFeedback | None = None\n",
    "    errors: Annotated[list[str], operator.add] = Field(default_factory=list)\n",
    "    iteration: int = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.types import RetryPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(usecwd=True))\n",
    "\n",
    "BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"http://a6k2.dgx:34000/v1\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZHCJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    api_key=API_KEY,\n",
    "    model=MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qnkX",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parsers\n",
    "planner_parser = PydanticOutputParser(pydantic_object=ResearchPlan)\n",
    "arxiv_parser = PydanticOutputParser(pydantic_object=ArxivFindings)\n",
    "web_parser = PydanticOutputParser(pydantic_object=WebFindings)\n",
    "synthesizer_parser = PydanticOutputParser(pydantic_object=ResearchReport)\n",
    "reviewer_parser = PydanticOutputParser(pydantic_object=ReviewFeedback)\n",
    "\n",
    "# Planner prompt\n",
    "PLANNER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            dedent(\n",
    "                \"\"\"\n",
    "        You are a research coordinator. Your task is to analyze the research topic and create a structured research plan.\n",
    "\n",
    "        Based on the user's topic, generate:\n",
    "        1. A well-formulated arXiv query\n",
    "        2. Multiple web search queries to cover different aspects\n",
    "        3. Key focus areas for the research\n",
    "        4. Expected number of sources\n",
    "\n",
    "        Be specific and thorough in your planning.\n",
    "\n",
    "        {format_instructions}\n",
    "        /no_think\n",
    "    \"\"\"\n",
    "            ).strip(),\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            dedent(\n",
    "                \"\"\"\n",
    "        Topic: {topic}\n",
    "        Max papers: {max_papers}\n",
    "        Max web results: {max_web_results}\n",
    "    \"\"\"\n",
    "            ).strip(),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ArXiv researcher prompt (for agent)\n",
    "ARXIV_SEARCH_PROMPT = dedent(\n",
    "    \"\"\"\n",
    "    You are an academic researcher specializing in analyzing arXiv papers.\n",
    "\n",
    "    Your task is to:\n",
    "    1. Search for relevant papers using the arXiv tool\n",
    "    2. Extract key information from each paper\n",
    "    3. Summarize the main contributions\n",
    "    4. Identify the most important papers for the research topic\n",
    "\n",
    "    Focus on recent, high-quality publications. Use the available tools to search arXiv.\n",
    "\"\"\"\n",
    ").strip()\n",
    "\n",
    "# ArXiv parser prompt\n",
    "ARXIV_PARSER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            dedent(\n",
    "                \"\"\"\n",
    "        You are a helpful assistant that extracts paper information from agent search results.\n",
    "        {format_instructions}\n",
    "        /no_think\n",
    "    \"\"\"\n",
    "            ).strip(),\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            dedent(\n",
    "                \"\"\"\n",
    "        Extract papers from these agent results:\n",
    "\n",
    "        {agent_output}\n",
    "    \"\"\"\n",
    "            ).strip(),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Web researcher prompt (for agent)\n",
    "WEB_SEARCH_PROMPT = dedent(\n",
    "    \"\"\"\n",
    "    You are a web researcher specializing in finding and analyzing online sources.\n",
    "\n",
    "    Your task is to:\n",
    "    1. Search the web using the DuckDuckGo tool\n",
    "    2. Extract and summarize relevant content\n",
    "    3. Assess the relevance and quality of each source\n",
    "    4. Provide concise summaries of key findings\n",
    "\n",
    "    Focus on authoritative and up-to-date sources. Use the available tools to search the web.\n",
    "\"\"\"\n",
    ").strip()\n",
    "\n",
    "# Web parser prompt\n",
    "WEB_PARSER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            dedent(\n",
    "                \"\"\"\n",
    "        You are a helpful assistant that extracts web source information from agent search results.\n",
    "        {format_instructions}\n",
    "        /no_think\n",
    "    \"\"\"\n",
    "            ).strip(),\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            dedent(\n",
    "                \"\"\"\n",
    "        Extract web sources from these agent results:\n",
    "\n",
    "        {agent_output}\n",
    "    \"\"\"\n",
    "            ).strip(),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Synthesizer prompt\n",
    "SYNTHESIZER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            dedent(\n",
    "                \"\"\"\n",
    "        You are a research analyst and technical writer.\n",
    "\n",
    "        Your task is to synthesize all collected research into a comprehensive report.\n",
    "\n",
    "        You should:\n",
    "        1. Identify and summarize key findings across all sources\n",
    "        2. Write a coherent summary of the research landscape\n",
    "        3. Highlight important papers and sources\n",
    "        4. Identify gaps in current research or understanding\n",
    "\n",
    "        Create a well-structured, informative research report.\n",
    "\n",
    "        {format_instructions}\n",
    "        /no_think\n",
    "    \"\"\"\n",
    "            ).strip(),\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            dedent(\n",
    "                \"\"\"\n",
    "        {context}\n",
    "    \"\"\"\n",
    "            ).strip(),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Reviewer prompt\n",
    "REVIEWER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            dedent(\n",
    "                \"\"\"\n",
    "        You are a critical reviewer and research quality assessor.\n",
    "\n",
    "        Your task is to evaluate the research report for:\n",
    "        1. Completeness - are all important aspects covered?\n",
    "        2. Quality - is the analysis thorough and well-reasoned?\n",
    "        3. Clarity - is the report well-written and understandable?\n",
    "        4. Missing aspects - what could be improved or added?\n",
    "\n",
    "        Provide a quality score (0.0-1.0) and decide if the report is approved or needs revision.\n",
    "        Be strict but fair in your assessment.\n",
    "\n",
    "        {format_instructions}\n",
    "        /no_think\n",
    "    \"\"\"\n",
    "            ).strip(),\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            dedent(\n",
    "                \"\"\"\n",
    "        {report_text}\n",
    "    \"\"\"\n",
    "            ).strip(),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_agent = create_agent(llm, [arxiv_search], system_prompt=ARXIV_SEARCH_PROMPT)\n",
    "web_agent = create_agent(llm, [search], system_prompt=WEB_SEARCH_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def planner_node(state):\n",
    "    logger.info(f\"planner: {state.query.topic}\")\n",
    "    messages = PLANNER_PROMPT.format_messages(\n",
    "        format_instructions=planner_parser.get_format_instructions(),\n",
    "        topic=state.query.topic,\n",
    "        max_papers=state.query.max_papers,\n",
    "        max_web_results=state.query.max_web_results,\n",
    "    )\n",
    "    response = await llm.ainvoke(messages)\n",
    "    plan = planner_parser.parse(response.content)\n",
    "    logger.info(f\"planner: {len(plan.web_queries)} queries\")\n",
    "    return {\"plan\": plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulZA",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def arxiv_researcher_node(state):\n",
    "    if not state.plan:\n",
    "        raise ValueError(\"No research plan available\")\n",
    "\n",
    "    query = state.plan.arxiv_query\n",
    "    max_papers = state.query.max_papers\n",
    "    logger.info(f\"arxiv_researcher: {query[:50]}...\")\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=f\"Search arXiv for: '{query}'. Find up to {max_papers} relevant papers.\"\n",
    "    )\n",
    "    agent_response = await arxiv_agent.ainvoke({\"messages\": [message]})\n",
    "    agent_output = agent_response[\"messages\"][-1].content\n",
    "\n",
    "    chain = ARXIV_PARSER_PROMPT | llm | arxiv_parser\n",
    "    parsed_result = await chain.ainvoke(\n",
    "        {\n",
    "            \"format_instructions\": arxiv_parser.get_format_instructions(),\n",
    "            \"agent_output\": agent_output,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    logger.info(f\"arxiv_researcher: {len(parsed_result.papers)} papers\")\n",
    "    return {\"arxiv_findings\": parsed_result.papers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def web_researcher_node(state):\n",
    "    if not state.plan:\n",
    "        raise ValueError(\"No research plan available\")\n",
    "\n",
    "    queries = \" AND \".join(state.plan.web_queries)\n",
    "    max_results = state.query.max_web_results\n",
    "    logger.info(f\"web_researcher: {queries[:50]}...\")\n",
    "\n",
    "    message = HumanMessage(\n",
    "        content=f\"Search web for: {queries}. Find up to {max_results} relevant sources.\"\n",
    "    )\n",
    "    agent_response = await web_agent.ainvoke({\"messages\": [message]})\n",
    "    agent_output = agent_response[\"messages\"][-1].content\n",
    "\n",
    "    chain = WEB_PARSER_PROMPT | llm | web_parser\n",
    "    parsed_result = await chain.ainvoke(\n",
    "        {\n",
    "            \"format_instructions\": web_parser.get_format_instructions(),\n",
    "            \"agent_output\": agent_output,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    logger.info(f\"web_researcher: {len(parsed_result.sources)} sources\")\n",
    "    return {\"web_findings\": parsed_result.sources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def synthesizer_node(state):\n",
    "    logger.info(f\"synthesizer: iter={state.iteration}\")\n",
    "    arxiv_summary = \"\\n\".join(\n",
    "        [\n",
    "            f\"- {p.title} by {', '.join(p.authors)}: {p.summary}\"\n",
    "            for p in state.arxiv_findings\n",
    "        ]\n",
    "    )\n",
    "    web_summary = \"\\n\".join(\n",
    "        [f\"- {s.title} ({s.url}): {s.content_summary}\" for s in state.web_findings]\n",
    "    )\n",
    "\n",
    "    feedback = \"\"\n",
    "    if state.review and state.review.suggestions:\n",
    "        feedback = \"Previous review feedback: \" + \", \".join(\n",
    "            state.review.suggestions\n",
    "        )\n",
    "\n",
    "    context = f\"\"\"Topic: {state.query.topic}\n",
    "\n",
    "ArXiv Papers:\n",
    "{arxiv_summary if arxiv_summary else \"No papers found\"}\n",
    "\n",
    "Web Sources:\n",
    "{web_summary if web_summary else \"No sources found\"}\n",
    "\n",
    "{feedback}\"\"\"\n",
    "\n",
    "    messages = SYNTHESIZER_PROMPT.format_messages(\n",
    "        format_instructions=synthesizer_parser.get_format_instructions(),\n",
    "        context=context,\n",
    "    )\n",
    "    response = await llm.ainvoke(messages)\n",
    "    report = synthesizer_parser.parse(response.content)\n",
    "    logger.info(f\"synthesizer: {len(report.key_findings)} findings\")\n",
    "    return {\"report\": report, \"iteration\": state.iteration + 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def reviewer_node(state):\n",
    "    if not state.report:\n",
    "        raise ValueError(\"No report available to review\")\n",
    "\n",
    "    logger.info(\"reviewer: evaluating report\")\n",
    "    report_text = f\"\"\"Report Summary: {state.report.summary}\n",
    "Key Findings: {\", \".join(state.report.key_findings)}\n",
    "Number of ArXiv papers: {len(state.arxiv_findings)}\n",
    "Number of Web sources: {len(state.web_findings)}\n",
    "Gaps identified: {\", \".join(state.report.gaps_identified)}\"\"\"\n",
    "\n",
    "    messages = REVIEWER_PROMPT.format_messages(\n",
    "        format_instructions=reviewer_parser.get_format_instructions(),\n",
    "        report_text=report_text,\n",
    "    )\n",
    "    response = await llm.ainvoke(messages)\n",
    "    review = reviewer_parser.parse(response.content)\n",
    "    logger.info(\n",
    "        f\"reviewer: approved={review.approved}, score={review.quality_score:.2f}\"\n",
    "    )\n",
    "    return {\"review\": review}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aLJB",
   "metadata": {
    "marimo": {
     "name": "*should_revise"
    }
   },
   "outputs": [],
   "source": [
    "def should_revise(state) -> str:\n",
    "    if state.review and not state.review.approved and state.iteration < 2:\n",
    "        return \"synthesizer\"\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = StateGraph(ResearchState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_policy = RetryPolicy(\n",
    "    max_attempts=3,\n",
    "    initial_interval=0.5,\n",
    "    backoff_factor=2.0,\n",
    "    jitter=True,\n",
    ")\n",
    "\n",
    "g.add_node(\"planner\", planner_node)\n",
    "g.add_node(\"arxiv_researcher\", arxiv_researcher_node, retry=retry_policy)\n",
    "g.add_node(\"web_researcher\", web_researcher_node, retry=retry_policy)\n",
    "g.add_node(\"synthesizer\", synthesizer_node)\n",
    "g.add_node(\"reviewer\", reviewer_node)\n",
    "\n",
    "g.set_entry_point(\"planner\")\n",
    "\n",
    "g.add_edge(\"planner\", \"arxiv_researcher\")\n",
    "g.add_edge(\"planner\", \"web_researcher\")\n",
    "\n",
    "g.add_edge(\"arxiv_researcher\", \"synthesizer\")\n",
    "g.add_edge(\"web_researcher\", \"synthesizer\")\n",
    "\n",
    "g.add_edge(\"synthesizer\", \"reviewer\")\n",
    "\n",
    "g.add_conditional_edges(\n",
    "    \"reviewer\", should_revise, {\"synthesizer\": \"synthesizer\", \"end\": END}\n",
    ")\n",
    "\n",
    "app = g.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AjVT",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.mermaid(app.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NCOB",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Demo Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqbW",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"Multi-agent systems with LLMs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_query = ResearchQuery(\n",
    "    topic=QUERY,\n",
    "    max_papers=3,\n",
    "    max_web_results=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = ResearchState(query=demo_query)\n",
    "result = await app.ainvoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dNNg",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
